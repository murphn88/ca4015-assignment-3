{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2bb911",
   "metadata": {},
   "source": [
    "# LightGCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86914599",
   "metadata": {},
   "source": [
    "Graphs are versatile data strucutres that can model complex elements and relationships. In this chapter I implement a Light Graph Convolution Network (LightGNC) to make recommendations. This work utilises a recommender library developed by Microsoft, instructions on installation can be found [here](https://github.com/microsoft/recommenders). The library provides utilities to aid common recommendation building tasks such as data cleaning, test/train splitting and the implementation of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7334c327",
   "metadata": {},
   "source": [
    "## Outline\n",
    "  1. Overview of LightGCN\n",
    "  1. Prepare data and hyper-parameters\n",
    "  1. Create and train model¶\n",
    "  1. Recommendations and evaluation¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a4cc32",
   "metadata": {},
   "source": [
    "## LightGCN Overview & Architecture\n",
    "\n",
    "Graph Convolution Network (GCNs) approaches involve semi-supervised learning on graph-structured data. Many real-world datasets come in the form of property graphs, yet until recently little effort has been devoted to the generalization of neural network models to graph structured datasets. GCNs are based on an efficient variant of convolutional neural networks. Convolutional architecure allow the to scale linearly and learn hidden layer representations.\n",
    "\n",
    "LightGCN is a simplified design of GCN, more concise and appropriate for recommenders. The model architecture is illustrated below.\n",
    "\n",
    "<img src=\"https://recodatasets.z20.web.core.windows.net/images/lightGCN-model.jpg\" width=\"600\">\n",
    "\n",
    "\n",
    "In Light Graph Convolution, only the normalized sum of neighbor embeddings is performed towards next layer; other operations like self-connection, feature transformation, and nonlinear activation are all removed, which largely simplifies GCNs. In Layer Combination,the embeddings at each layer are summed over to achieve the final representations.\n",
    "\n",
    "### Light Graph Convolution (LGC)\n",
    "\n",
    "In LightGCN, a simple weighted sum aggregator is utilised. The graph convolution operation in LightGCN is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\mathbf{e}_{u}^{(k+1)}=\\sum_{i \\in \\mathcal{N}_{u}} \\frac{1}{\\sqrt{\\left|\\mathcal{N}_{u}\\right|} \\sqrt{\\left|\\mathcal{N}_{i}\\right|}} \\mathbf{e}_{i}^{(k)} \\\\\n",
    "\\mathbf{e}_{i}^{(k+1)}=\\sum_{u \\in \\mathcal{N}_{i}} \\frac{1}{\\sqrt{\\left|\\mathcal{N}_{i}\\right|} \\sqrt{\\left|\\mathcal{N}_{u}\\right|}} \\mathbf{e}_{u}^{(k)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The symmetric normalization term $\\frac{1}{\\sqrt{\\left|\\mathcal{N}_{u}\\right|} \\sqrt{\\left|\\mathcal{N}_{i}\\right|}}$ follows the design of standard GCN, which can avoid the scale of embeddings increasing with graph convolution operations.\n",
    "\n",
    "\n",
    "### Layer Combination and Model Prediction\n",
    "\n",
    "The embeddings at the 0-th layer are the only trainable parameters, i.e., $\\mathbf{e}_{u}^{(0)}$ for all users and $\\mathbf{e}_{i}^{(0)}$ for all items. After $K$ layer, the embeddings are further combined at each layer to arrive at the final representation of a user (an item):\n",
    "\n",
    "$$\n",
    "\\mathbf{e}_{u}=\\sum_{k=0}^{K} \\alpha_{k} \\mathbf{e}_{u}^{(k)} ; \\quad \\mathbf{e}_{i}=\\sum_{k=0}^{K} \\alpha_{k} \\mathbf{e}_{i}^{(k)}\n",
    "$$\n",
    "\n",
    "where $\\alpha_{k} \\geq 0$ denotes the importance of the $k$-th layer embedding in constituting the final embedding. In our experiments, we set $\\alpha_{k}$ uniformly as $1 / (K+1)$.\n",
    "\n",
    "The model prediction is defined as the inner product of user and item final representations:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{u i}=\\mathbf{e}_{u}^{T} \\mathbf{e}_{i}\n",
    "$$\n",
    "\n",
    "which is used as the ranking score for recommendation generation.\n",
    "\n",
    "\n",
    "### Matrix Form\n",
    "\n",
    "Let the user-item interaction matrix be $\\mathbf{R} \\in \\mathbb{R}^{M \\times N}$ where $M$ and $N$ denote the number of users and items, respectively, and each entry $R_{ui}$ is 1 if $u$ has interacted with item $i$ otherwise 0. The adjacency matrix of the user-item graph is \n",
    "\n",
    "$$\n",
    "\\mathbf{A}=\\left(\\begin{array}{cc}\n",
    "\\mathbf{0} & \\mathbf{R} \\\\\n",
    "\\mathbf{R}^{T} & \\mathbf{0}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Let the 0-th layer embedding matrix be $\\mathbf{E}^{(0)} \\in \\mathbb{R}^{(M+N) \\times T}$, where $T$ is the embedding size. Then we can obtain the matrix equivalent form of LGC as:\n",
    "\n",
    "$$\n",
    "\\mathbf{E}^{(k+1)}=\\left(\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}}\\right) \\mathbf{E}^{(k)}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{D}$ is a $(M+N) \\times(M+N)$ diagonal matrix, in which each entry $D_{ii}$ denotes the number of nonzero entries in the $i$-th row vector of the adjacency matrix $\\mathbf{A}$ (also named as degree matrix). Lastly, we get the final embedding matrix used for model prediction as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{E} &=\\alpha_{0} \\mathbf{E}^{(0)}+\\alpha_{1} \\mathbf{E}^{(1)}+\\alpha_{2} \\mathbf{E}^{(2)}+\\ldots+\\alpha_{K} \\mathbf{E}^{(K)} \\\\\n",
    "&=\\alpha_{0} \\mathbf{E}^{(0)}+\\alpha_{1} \\tilde{\\mathbf{A}} \\mathbf{E}^{(0)}+\\alpha_{2} \\tilde{\\mathbf{A}}^{2} \\mathbf{E}^{(0)}+\\ldots+\\alpha_{K} \\tilde{\\mathbf{A}}^{K} \\mathbf{E}^{(0)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\tilde{\\mathbf{A}}=\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}}$ is the symmetrically normalized matrix.\n",
    "\n",
    "### Model Training\n",
    "\n",
    "Bayesian Personalized Ranking (BPR) loss is used. BPR is a a pairwise loss that encourages the prediction of an observed entry to be higher than its unobserved counterparts:\n",
    "\n",
    "$$\n",
    "L_{B P R}=-\\sum_{u=1}^{M} \\sum_{i \\in \\mathcal{N}_{u}} \\sum_{j \\notin \\mathcal{N}_{u}} \\ln \\sigma\\left(\\hat{y}_{u i}-\\hat{y}_{u j}\\right)+\\lambda\\left\\|\\mathbf{E}^{(0)}\\right\\|^{2}\n",
    "$$\n",
    "\n",
    "Where $\\lambda$ controls the $L_2$ regularization strength.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04078a02",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d4a2c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.models.deeprec.models.graphrec.lightgcn import LightGCN\n",
    "from recommenders.models.deeprec.DataModel.ImplicitCF import ImplicitCF\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.datasets.python_splitters import python_stratified_split\n",
    "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    "from recommenders.utils.constants import SEED as DEFAULT_SEED\n",
    "from recommenders.models.deeprec.deeprec_utils import prepare_hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf0bc95",
   "metadata": {},
   "source": [
    "## Read in Data & Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2fe9caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "listens = pd.read_csv('.\\\\data\\\\processed\\\\listens.csv',index_col=0)\n",
    "artists = pd.read_csv('.\\\\data\\\\processed\\\\artists.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c8d826f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_dict = pd.Series(artists.name,index=artists.id).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7b9b2178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>listenCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>3.047442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.047442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>3.047442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  artistID  listenCount\n",
       "0       0        45     3.047442\n",
       "1       0        46     3.047442\n",
       "2       0        47     3.047442"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listens.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51f0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "LISTENS_DATA_SIZE = '100k'\n",
    "\n",
    "# Model parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "SEED = DEFAULT_SEED  # Set None for non-deterministic results\n",
    "\n",
    "yaml_file = \"./lightgcn.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb84289",
   "metadata": {},
   "source": [
    "## LightGCN Implementation\n",
    "\n",
    "### Split Data\n",
    "We split the full dataset into a train and test dataset to evaluate performance of the algorithm against a held-out set not seen during training. Because SAR generates recommendations based on user preferences, all users that are in the test set must also exist in the training set. We can use the provided python_stratified_split function which holds out a percentage of items from each user, but ensures all users are in both train and test datasets. We will use a 75/25 train/test split. I considered keeping the split at for consistency with the matrix factorization and softmax models. However,this method relies heavily on users' historic listening records and is being split in a different manner so I decided against it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "83d6a780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>3.047442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.047442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>3.047442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>3.047442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>3.047442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID    rating\n",
       "0       0      45  3.047442\n",
       "1       0      46  3.047442\n",
       "2       0      47  3.047442\n",
       "3       0      48  3.047442\n",
       "4       0      49  3.047442"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = listens\n",
    "df = df.rename(columns={'listenCount': 'rating', 'artistID':'itemID'})\n",
    "# listens['timestamp'] = np.nan\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "39e8c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_stratified_split(df, ratio=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a84119",
   "metadata": {},
   "source": [
    "### Process data\n",
    "\n",
    "`ImplicitCF` is a class that intializes and loads data for the training process. During the initialization of this class, user IDs and item IDs are reindexed, ratings greater than zero are converted into implicit positive interaction, and an adjacency matrix of the user-item graph is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6e132662",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImplicitCF(train=train, test=test, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636e7a4",
   "metadata": {},
   "source": [
    "### Prepare hyper-parameters\n",
    "\n",
    "Parameters can be set for ths LightGNC. To save time on tuning parameters we will use the prepared paramemters that can be found in `yaml_file`. `prepare_hparams` reads in the yaml file and prepares a full set of parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2a69477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(yaml_file,\n",
    "                          n_layers=3,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          epochs=EPOCHS,\n",
    "                          learning_rate=0.005,\n",
    "                          eval_epoch=5,\n",
    "                          top_k=TOP_K,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5310d118",
   "metadata": {},
   "source": [
    "### Create and train model\n",
    "\n",
    "With data and parameters prepared, we can create and train the LightGCN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d92c1867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already create adjacency matrix.\n",
      "Already normalize adjacency matrix.\n",
      "Using xavier initialization.\n"
     ]
    }
   ],
   "source": [
    "model = LightGCN(hparams, data, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "8711d76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (train)9.2s: train loss = 0.42353 = (mf)0.42337 + (embed)0.00016\n",
      "Epoch 2 (train)9.1s: train loss = 0.19883 = (mf)0.19836 + (embed)0.00047\n",
      "Epoch 3 (train)8.4s: train loss = 0.15302 = (mf)0.15242 + (embed)0.00059\n",
      "Epoch 4 (train)9.4s: train loss = 0.12323 = (mf)0.12253 + (embed)0.00070\n",
      "Epoch 5 (train)8.7s + (eval)1.1s: train loss = 0.10586 = (mf)0.10505 + (embed)0.00080, recall = 0.09133, ndcg = 0.11955, precision = 0.10797, map = 0.04692\n",
      "Epoch 6 (train)9.0s: train loss = 0.09377 = (mf)0.09288 + (embed)0.00089\n",
      "Epoch 7 (train)9.1s: train loss = 0.08220 = (mf)0.08122 + (embed)0.00099\n",
      "Epoch 8 (train)8.5s: train loss = 0.07451 = (mf)0.07344 + (embed)0.00107\n",
      "Epoch 9 (train)8.6s: train loss = 0.06745 = (mf)0.06629 + (embed)0.00116\n",
      "Epoch 10 (train)8.7s + (eval)0.9s: train loss = 0.05959 = (mf)0.05835 + (embed)0.00124, recall = 0.11003, ndcg = 0.14639, precision = 0.13027, map = 0.05780\n",
      "Epoch 11 (train)9.1s: train loss = 0.05491 = (mf)0.05359 + (embed)0.00132\n",
      "Epoch 12 (train)8.5s: train loss = 0.04991 = (mf)0.04851 + (embed)0.00139\n",
      "Epoch 13 (train)8.7s: train loss = 0.04857 = (mf)0.04710 + (embed)0.00147\n",
      "Epoch 14 (train)8.7s: train loss = 0.04412 = (mf)0.04257 + (embed)0.00154\n",
      "Epoch 15 (train)8.7s + (eval)0.9s: train loss = 0.04175 = (mf)0.04014 + (embed)0.00161, recall = 0.12334, ndcg = 0.16331, precision = 0.14599, map = 0.06553\n",
      "Epoch 16 (train)8.9s: train loss = 0.03916 = (mf)0.03748 + (embed)0.00168\n",
      "Epoch 17 (train)9.0s: train loss = 0.03575 = (mf)0.03400 + (embed)0.00175\n",
      "Epoch 18 (train)9.0s: train loss = 0.03453 = (mf)0.03272 + (embed)0.00182\n",
      "Epoch 19 (train)8.6s: train loss = 0.03400 = (mf)0.03212 + (embed)0.00188\n",
      "Epoch 20 (train)9.2s + (eval)1.0s: train loss = 0.03236 = (mf)0.03042 + (embed)0.00194, recall = 0.13128, ndcg = 0.17543, precision = 0.15550, map = 0.07098\n",
      "Epoch 21 (train)9.1s: train loss = 0.03115 = (mf)0.02916 + (embed)0.00199\n",
      "Epoch 22 (train)9.4s: train loss = 0.02957 = (mf)0.02751 + (embed)0.00205\n",
      "Epoch 23 (train)9.0s: train loss = 0.02818 = (mf)0.02608 + (embed)0.00211\n",
      "Epoch 24 (train)8.7s: train loss = 0.02638 = (mf)0.02422 + (embed)0.00216\n",
      "Epoch 25 (train)8.9s + (eval)0.9s: train loss = 0.02666 = (mf)0.02445 + (embed)0.00221, recall = 0.13709, ndcg = 0.18341, precision = 0.16240, map = 0.07455\n",
      "Epoch 26 (train)9.2s: train loss = 0.02509 = (mf)0.02282 + (embed)0.00227\n",
      "Epoch 27 (train)8.9s: train loss = 0.02325 = (mf)0.02093 + (embed)0.00232\n",
      "Epoch 28 (train)8.5s: train loss = 0.02202 = (mf)0.01965 + (embed)0.00237\n",
      "Epoch 29 (train)9.0s: train loss = 0.02138 = (mf)0.01896 + (embed)0.00243\n",
      "Epoch 30 (train)9.0s + (eval)0.9s: train loss = 0.02225 = (mf)0.01978 + (embed)0.00248, recall = 0.14030, ndcg = 0.18926, precision = 0.16612, map = 0.07737\n",
      "Epoch 31 (train)8.8s: train loss = 0.02231 = (mf)0.01980 + (embed)0.00251\n",
      "Epoch 32 (train)8.7s: train loss = 0.02038 = (mf)0.01781 + (embed)0.00256\n",
      "Epoch 33 (train)8.7s: train loss = 0.02028 = (mf)0.01766 + (embed)0.00261\n",
      "Epoch 34 (train)8.7s: train loss = 0.01879 = (mf)0.01614 + (embed)0.00266\n",
      "Epoch 35 (train)8.5s + (eval)0.9s: train loss = 0.01845 = (mf)0.01575 + (embed)0.00271, recall = 0.14525, ndcg = 0.19626, precision = 0.17180, map = 0.08061\n",
      "Epoch 36 (train)9.0s: train loss = 0.01845 = (mf)0.01569 + (embed)0.00275\n",
      "Epoch 37 (train)8.8s: train loss = 0.01814 = (mf)0.01535 + (embed)0.00279\n",
      "Epoch 38 (train)8.7s: train loss = 0.01757 = (mf)0.01474 + (embed)0.00283\n",
      "Epoch 39 (train)8.8s: train loss = 0.01699 = (mf)0.01412 + (embed)0.00287\n",
      "Epoch 40 (train)8.4s + (eval)0.9s: train loss = 0.01673 = (mf)0.01382 + (embed)0.00291, recall = 0.14834, ndcg = 0.20013, precision = 0.17515, map = 0.08211\n",
      "Epoch 41 (train)9.8s: train loss = 0.01574 = (mf)0.01279 + (embed)0.00295\n",
      "Epoch 42 (train)8.5s: train loss = 0.01637 = (mf)0.01339 + (embed)0.00298\n",
      "Epoch 43 (train)8.8s: train loss = 0.01714 = (mf)0.01412 + (embed)0.00302\n",
      "Epoch 44 (train)8.9s: train loss = 0.01568 = (mf)0.01262 + (embed)0.00305\n",
      "Epoch 45 (train)8.9s + (eval)1.1s: train loss = 0.01618 = (mf)0.01309 + (embed)0.00309, recall = 0.15104, ndcg = 0.20393, precision = 0.17870, map = 0.08413\n",
      "Epoch 46 (train)9.4s: train loss = 0.01409 = (mf)0.01097 + (embed)0.00313\n",
      "Epoch 47 (train)8.8s: train loss = 0.01500 = (mf)0.01183 + (embed)0.00316\n",
      "Epoch 48 (train)9.5s: train loss = 0.01427 = (mf)0.01107 + (embed)0.00319\n",
      "Epoch 49 (train)8.3s: train loss = 0.01427 = (mf)0.01104 + (embed)0.00323\n",
      "Epoch 50 (train)10.5s + (eval)1.2s: train loss = 0.01370 = (mf)0.01044 + (embed)0.00326, recall = 0.15244, ndcg = 0.20657, precision = 0.18019, map = 0.08504\n",
      "Took 455.2403181999998 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model.fit()\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef01aa",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225fa39d",
   "metadata": {},
   "source": [
    "`recommend_k_items` produces k artist recommendations for each user passed to the function. `remove_seen=True` removes the artists already listened to by the user. We will produce recommendations using the trained model on instances from the test set as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "90fb9001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>prediction</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>992</td>\n",
       "      <td>12.571548</td>\n",
       "      <td>Pet Shop Boys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>11.216814</td>\n",
       "      <td>Michael Jackson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>11.112077</td>\n",
       "      <td>a-ha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>593</td>\n",
       "      <td>10.635868</td>\n",
       "      <td>David Bowie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1005</td>\n",
       "      <td>10.495359</td>\n",
       "      <td>Erasure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  prediction             name\n",
       "0       0     992   12.571548    Pet Shop Boys\n",
       "1       0     151   11.216814  Michael Jackson\n",
       "2       0     181   11.112077             a-ha\n",
       "3       0     593   10.635868      David Bowie\n",
       "4       0    1005   10.495359          Erasure"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_scores = model.recommend_k_items(test, top_k=TOP_K, remove_seen=True)\n",
    "top_scores = topk_scores\n",
    "top_scores['name'] = topk_scores.itemID.map(artist_dict)\n",
    "top_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "45ede05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_recommendations(user):\n",
    "    listened_to = train[train.userID == user].sort_values('rating',ascending=False)\n",
    "    listened_to['name'] = listened_to.itemID.map(artist_dict)\n",
    "    listened_to = listened_to.head(10).name\n",
    "    print('User ' + str(user) + ' most listened to artists...')\n",
    "    print('\\n'.join(listened_to) + '\\n')\n",
    "    \n",
    "    topk_scores_recs = topk_scores[topk_scores.userID == user].sort_values('prediction',ascending=False).name\n",
    "    print('User ' + str(user) + ' recommendations...')\n",
    "    print('\\n'.join(topk_scores_recs.tolist()))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "51101136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 500 most listened to artists...\n",
      "Christina Aguilera\n",
      "John Mayer\n",
      "Chico Buarque\n",
      "Sarah Brightman\n",
      "Oasis\n",
      "The Beatles\n",
      "Lady Gaga\n",
      "Adele\n",
      "Justin Timberlake\n",
      "Paul McCartney\n",
      "\n",
      "User 500 recommendations...\n",
      "Britney Spears\n",
      "BeyoncÃ©\n",
      "Kylie Minogue\n",
      "P!nk\n",
      "Coldplay\n",
      "Amy Winehouse\n",
      "Black Eyed Peas\n",
      "Mariah Carey\n",
      "Ke$ha\n",
      "Kelly Clarkson\n"
     ]
    }
   ],
   "source": [
    "user_recommendations(user=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "240a7865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 300 most listened to artists...\n",
      "Van Halen\n",
      "KISS\n",
      "Iron Maiden\n",
      "Black Sabbath\n",
      "Leaves' Eyes\n",
      "Epica\n",
      "The Agonist\n",
      "Five Finger Death Punch\n",
      "AC/DC\n",
      "Deadstar Assembly\n",
      "\n",
      "User 300 recommendations...\n",
      "System of a Down\n",
      "Metallica\n",
      "Korn\n",
      "In Flames\n",
      "Megadeth\n",
      "Rammstein\n",
      "Bullet for My Valentine\n",
      "HIM\n",
      "Judas Priest\n",
      "Pantera\n"
     ]
    }
   ],
   "source": [
    "user_recommendations(user=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ee0ad",
   "metadata": {},
   "source": [
    "At a glance, the recommendation system appears to work extremely well. User 500 has pretty broad and genric music tastes, yet each recommended artist makes sense. User 300 appears to have more specified music interests. Most of user 300's top listened to artists are rock/heavy metal bands from the 70s/80s. The recommendations are also mainly rock/heavy metal bands from the same time period. Across both users, all recommendations appear relevant and potentially useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cf1327",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With `topk_scores` (k=10) predicted by the model, we can evaluate how LightGCN performs on the test set. We will use four evaluation metrics:\n",
    "1. Mean Average Precision (MAP)\n",
    "1. Normalized Discounted Cumulative Gain (NDCGG)\n",
    "1. Precision at 10\n",
    "1. Recall at 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "31b18d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:\t0.023575\n",
      "NDCG:\t0.082716\n",
      "Precision@K:\t0.087785\n",
      "Recall@K:\t0.074625\n"
     ]
    }
   ],
   "source": [
    "eval_map = map_at_k(test, topk_scores, k=TOP_K)\n",
    "eval_ndcg = ndcg_at_k(test, topk_scores, k=TOP_K)\n",
    "eval_precision = precision_at_k(test, topk_scores, k=TOP_K)\n",
    "eval_recall = recall_at_k(test, topk_scores, k=TOP_K)\n",
    "\n",
    "print(\"MAP:\\t%f\" % eval_map,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg,\n",
    "      \"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e02b3d5",
   "metadata": {},
   "source": [
    "These results are promising and they back up the assumption made from looking at two users' recommendations that the model works. Although, the test split was different than the test splits used to evaluate matrix factorization and softmax, this model's precision is still almost 10 times higher. It appears that this is the superior recommendation system and that we have managed to beat the standard of the initial matrix factorization model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3a590",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a29d6",
   "metadata": {},
   "source": [
    "LightGCN is a light weight and efficient form of a GCN that can be quickly built, trained, and evaluated on this dataset without the need for a GPU. Even without tuning the hyperparameters, the results and recommendations produced by this model are impressive. Here, we have produced a relevant and potentially useful artist recommendation system. The [recommender library](https://github.com/microsoft/recommenders) was also extremely useful and appropiate for our objective of building an artist recommender system using our Last.fm dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}